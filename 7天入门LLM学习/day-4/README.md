# day-4

## 经验总结

方法选型：

- 直接推理：结合提示词工程
  - 适用场景：视模型本身的能力而定，在采用该方式之前需要对现有模型针对自己的业务领域进行较为充分的评估。
  - 准确性：由于是原始模型只接受了通用知识的训练，因此在特定领域的场景下可能存在胡编乱造的可能性（幻觉问题）。使用者需要注意自己的专业场景下是否使用该通用模型能解决所有问题，一般建议直接试用该模型给出模型能力的具体评估。
  - 成本：开发成本较低。如果是开源模型，需要选用合适的硬件及推理方式。这部分在我们教程中的推理章节会有讲解。如果是闭源调用，只需要使用对应模型的接口API即可
  - 缺点：由于模型没有经过针对特有领域的知识，因此效果会比较不可控。比如，在评测时模型表现尚可，但在实际使用中发现模型出现了严重的幻觉和知识匮乏问题，如果是闭源调用则该问题会比较难以解决（可能涉及到工程架构改变），如果是开源模型可以考虑使用训练和RAG的方式解决
- 训练：

## py-torch

### 梯度的定义
梯度是一个向量，表示多元函数在某一点处的变化率和方向。对于一个标量函数 $ f(x_1, x_2, ..., x_n) $，其梯度记为：

$$
\nabla f = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n} \right]
$$

- 每个分量是函数对某个变量的偏导数。
- 梯度的方向是函数值增长最快的方向，大小是变化率。

---

### 在深度学习中的意义
在深度学习中，梯度用于优化模型参数（如权重和偏置）。通过计算损失函数对参数的梯度，可以使用梯度下降等优化算法更新参数，使损失函数逐渐减小。

---

### 示例解释
在你的代码中：
```python
a = torch.tensor([1.], requires_grad=True)
b = torch.tensor([2.], requires_grad=True)
c = a * b
```
- 定义了两个张量 `a` 和 `b`，并设置 `requires_grad=True`，表示需要计算它们的梯度。
- 计算 `c = a * b`，即 $ c = a \cdot b $。
- 调用 `c.backward()` 时，PyTorch 自动计算 $ c $ 对 $ a $ 和 $ b $ 的偏导数：
  - $ \frac{\partial c}{\partial a} = b $
  - $ \frac{\partial c}{\partial b} = a $

最终结果：
- `a.grad` 是 $ \frac{\partial c}{\partial a} $，值为 `[2.]`。
- `b.grad` 是 $ \frac{\partial c}{\partial b} $，值为 `[1.]`。

---

### 总结
梯度是函数变化率的核心概念，在深度学习中用于指导参数更新，优化模型性能。
