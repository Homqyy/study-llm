# LLM 量化

用整数计算代替浮点数计算的方法就是量化

基本原理：根据每个tensor的浮点型最大值和最小值，将其映射为一个固定范围的整形数值集合，比如[-127~127]。

> 假设一个简单的公式：qweight=round(weight/scale)，其中qweight代表量化后权重，weight代表量化前权重，scale代表缩放因子，可以看到在进行缩放后为了将浮点型转换为整数过程中增加了round操作丢失了小数部分。在后续计算或反量化为浮点型时存在无法完全还原的情况，这就是精度损失。

根据量化发生的步骤，可以划分为：

- PTQ（训练后量化，或离线量化）：
    - data-free：不需要数据集进行校验，直接对模型进行量化
    - calibration：根据少量真实数据进行统计分析并对量化因子进行额外校准，但耗费的时间更长
- QAT（训练时量化）：
    - 会先在待量化的算子上增加一个伪量化结构，并在训练时模拟量化过程并实时更新计算量化因子（类似反向传播过程）及原始权重
    - QAT由于较为复杂一般作为辅助措施存在，用于改进PTQ量化的技术手段

根据量化方法，可以划分为：

- 线性量化
    - 对称量化：
    - 非对称量化：$qweight=round(weight/scale + zeropoint)$
        - 用于解决weight分布不均问题
- 非线性量化

根据量化粒度，可以划分为：

- 逐层量化：每层使用一套量化因子
- 逐组量化：在每层中按照group使用一套量化因子
- 逐通道量化：按channel划分量化因子

根据量化最大值的阈值区分，可以划分为：

- 饱和量化（Term: Saturation）：饱和量化会计算一个中间值以计算出量化因子，因此会舍弃一部分不重要数据，将重要数据尽量均匀的分布到量化数值范围内。
- 不饱和量化： 不饱和量化按照浮点数最大值和量化后最大值的比例计算量化因子，由于原始weight的非均匀性会导致某些整形数值范围存在权重空缺

根据量化后的比特数划分，可以划分为：

- 2bit量化
- 4bit量化
- 8bit量化

## 量化方法

### AutoGPTQ

该库需要引入额外的校准数据集进行量化校准。相比bitsandbytes量化精度较高，推理速度较快，但训练后不支持合并adapter。

### Bitsandbytes

bitsandbytes是一种data-free的量化库。该量化方法速度较快（因为其不需要数据校准），因此可以在模型加载时动态量化，且该方法训练速度较快，因此训练兼容性较好，一般用于QLoRA训练中，且训练后可以合并adapter。但由于其没有数据校准过程，因此精度较AutoGPTQ较低。

### GGML

GGML和GGUF是GGML C++推理库的两种量化格式，其中GGUF格式较新，可以保留模型版本等其他自定义信息。这两种格式也是PTQ形式的量化算法，但GGML和GGUF格式的量化算法更适配于CPU推理，因此在CPU上运行更快，而GPTQ量化对GPU更加友好，两者的推理精度相仿。因此，***.cpp类型使用了GGML推理库的推理框架都更适配于CPU推理**。

### AWQ

AWQ量化方式假设不是所有权重都影响模型性能，因此在量化过程中会跳过一部分重要权重以减轻量化过程中的精度损失。因此在和GPTQ量化保持类似推理速度的同时可以具备更好的精度。

目前VLLM对AWQ的支持较好, 可以考虑在推理加速时使用AWQ量化方式。

## 推理

一些推理方法：

- Greedy Search 贪婪搜索方式。按照前面的讲解，模型会按照词表尺寸生成概率。贪婪方式会不断选择生成概率最大的token。该方法由于无脑选择了最大概率，因此模型会倾向于生成重复的文字，一般实际应用中很少使用
- Beam Search 和贪婪方式的区别在于，beam search会选择概率最大的k个。在生成下一个token时，每个前序token都会生成k个，这样整体序列就有k^2个，从这些序列中选择组合概率最大的k个，并递归地执行下去。k在beam search算法中被称为beam_size
- Sample 随机采样方式。按照词表每个token的概率采样一个token出来。这个方式多样性更强，是目前主流的生成方式。

重要推理超参数：

- dosample：布尔类型。是否使用随机采样方式运行推理，如果设置为False，则使用beamsearch方式
- temperature：大于等于零的浮点数。如果T取值为0，则效果类似argmax，此时推理几乎没有随机性；取值为正无穷时接近于取平均。一般temperature取值介于[0, 1]之间。取值越高输出效果越随机。如果该问答只存在确定性答案，则T值设置为0。反之设置为大于0。
- top_k：大于0的正整数。从k个概率最大的结果中进行采样。k越大多样性越强，越小确定性越强。一般设置为20~100之间。
    - 实际实验中可以先从100开始尝试，逐步降低top_k直到效果达到最佳。
- top_p：大于0的浮点数。使所有被考虑的结果的概率和大于p值，p值越大多样性越强，越小确定性越强。一般设置0.7~0.95之间。
    - 实际实验中可以先从0.95开始降低，直到效果达到最佳。
    - topp比topk更有效，应优先调节这个参数。
- repetition_penalty： 大于等于1.0的浮点数。如何惩罚重复token，默认1.0代表没有惩罚。

KVCache：

推理时的Q是单token tensor，但K和V都是包含了所有历史token tensor的长序列，因此KV是可以使用前序计算的中间结果的，这部分的缓存就是KVCache，其显存占用非常巨大。

VLLM：

## FAQ

### adapter如何跟与训练的语言模型配合？在架构上。比如原本的语言模型是一个 transformer的decoder

当语言模型是基于Transformer的Decoder架构时，Adapter通常可以通过以下几种方式与预训练语言模型配合：
- **在Transformer层间插入**：可以在Decoder的不同层之间插入Adapter模块。例如，在每一层的Self - Attention层之后或Feed - Forward层之后插入。当输入序列经过Decoder的各层时，先经过常规的Transformer层计算，然后将结果输入到Adapter中进行进一步的特征提取和转换。Adapter可以学习到特定任务或领域的特征，对Transformer层输出的特征进行调整和优化，以更好地适应具体任务需求。
- **作为额外的并行分支**：将Adapter构建为与Decoder的常规路径并行的分支。输入序列同时通过常规的Transformer Decoder路径和Adapter分支进行处理。在Adapter分支中，可以使用一些特殊的网络结构，如小型的卷积神经网络或多层感知机等，对输入进行不同方式的特征提取。然后，将Adapter分支的输出与常规Decoder路径的输出进行融合，例如通过相加或拼接等操作，再将融合后的结果继续传递到下一层进行处理。这样，Adapter可以为模型引入额外的特征信息，丰富模型的表示能力。
- **替换部分模块**：可以考虑替换Decoder中的一些相对较小的模块或子结构为Adapter。比如，对于一些不太关键的Feed - Forward层或注意力机制中的某些组件，可以用Adapter来替代。这样，在不改变整体模型架构的基础上，利用Adapter的灵活性来更好地适应特定任务。被替换的模块通常是那些对任务特异性比较敏感，通过Adapter可以更有效地学习到任务相关特征的部分。

通过这些方式，Adapter能够与基于Transformer Decoder的预训练语言模型紧密配合，在保持预训练模型大部分通用能力的同时，针对具体任务进行高效的适配和优化，从而提高模型在特定任务上的性能。