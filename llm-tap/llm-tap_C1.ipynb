{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本概念\n",
    "\n",
    "## 编码器（Encoder）\n",
    "\n",
    "## 解码器（Decoder）\n",
    "\n",
    "## 残差连接(Residual Connection)\n",
    "\n",
    "残差连接主要是指使用一条直连通道直接将对应子层的输入连接到输出上去，从而避免由于网络过深在优化过程中潜在的梯度消失问题。\n",
    "\n",
    "$$\n",
    "x^{l+1} = f(x^l) + x^l\n",
    "$$\n",
    "\n",
    "## 层一归化（Layer Normalization）\n",
    "\n",
    "其中 µ 和 σ 分别表示均值和方差，用于将数据平移缩放到均值为 0，方差为 1 的标准分布，α 和 b\n",
    "是可学习的参数。层归一化技术可以有效地缓解优化过程中潜在的不稳定、收敛速度慢等问题。\n",
    "\n",
    "$$\n",
    "\\text{LN}(x) = \\alpha \\cdot \\frac{x - \\mu}{\\sigma} + b\n",
    "$$\n",
    "\n",
    "## 全连接层（Fully Connected Layer）\n",
    "\n",
    "全连接层是神经网络中的一种基本层，它将输入数据通过一个线性变换和非线性激活函数映射到输出空间。\n",
    "\n",
    "## 位置感知前馈层（Position-wise Feed-Forward Layer）\n",
    "\n",
    "\n",
    "\n",
    "## 掩码多头注意力（Masked Multi-Head Attention）\n",
    "\n",
    "## 多头交叉注意力（Multi-Head Cross Attention）\n",
    "\n",
    "## 有监督微调（Supervised Fine-Tuning, SFT）\n",
    "\n",
    "## 指令微调（Instruction Tuning, IT）"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
