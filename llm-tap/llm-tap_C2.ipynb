{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187b7762-a438-4971-9665-c969b2b25180",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "\n",
    "modelscope数据集使用可以参阅：<https://modelscope.cn/docs/sdk/dataset>\n",
    "\n",
    "## install dependencies:\n",
    "\n",
    "```bash\n",
    "pip install datasets transformers sentencepiece modelscope[framework]\n",
    "\n",
    "# windows\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "# linux\n",
    "pip install torch torchvision torchaudio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1386afb6-a5a2-4b96-8160-2c7d17b07033",
   "metadata": {},
   "source": [
    "## prepare datasets: exec shell script \n",
    "\n",
    "```bash\n",
    "modelscope download --dataset BAAI/IndustryCorpus2 --local_dir ./datasets/ --include computer_programming_code/*\n",
    "```\n",
    "\n",
    "具体可以执行：\n",
    "\n",
    "```bash\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595fd8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''\n",
    "load dataset\n",
    "'''\n",
    "from modelscope import MsDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "files = [\n",
    "    './datasets/computer_programming_code/chinese/low/rank_00163.parquet',\n",
    "    './datasets/computer_programming_code/english/low/rank_01598.parquet',\n",
    "    './datasets/computer_programming_code/english/low/rank_01599.parquet',\n",
    "    './datasets/computer_programming_code/english/low/rank_01600.parquet',\n",
    "    './datasets/computer_programming_code/english/low/rank_01601.parquet',\n",
    "    './datasets/computer_programming_code/english/low/rank_01602.parquet',\n",
    "]\n",
    "\n",
    "ds = MsDataset.load(dataset_name='parquet', data_files=files)\n",
    "\n",
    "hf_ds: Dataset = ds.to_hf_dataset()\n",
    "print('hf_ds:', hf_ds)\n",
    "\n",
    "# only keep text column\n",
    "hf_ds = hf_ds.remove_columns([col for col in hf_ds.column_names if col != \"text\"])\n",
    "# split into train and test sets with 90% training and 10% testing\n",
    "d = hf_ds.train_test_split(test_size=0.1)\n",
    "print('train and test dataset:', d)\n",
    "\n",
    "hf_ds = None # free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb56cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d[\"train\"]: Dataset({\n",
      "    features: ['text', 'alnum_ratio', 'avg_line_length', 'char_rep_ratio', 'flagged_words_ratio', 'max_line_length', 'num_words', 'perplexity', 'quality_score', 'special_char_ratio', 'word_rep_ratio', '_id', 'industry_type'],\n",
      "    num_rows: 1862001\n",
      "})\n",
      "d[\"train\"][text][0]: Q:\n",
      "\n",
      "How to collect into a Map forming a List in value when duplicate keys in streams, in Java 8\n",
      "\n",
      "I have a stream of elements in the form of either 2-D array or EntrySet. I need these to be collected in a Map. Now the issue is the stream of elements can have duplicate elements. Let's say I want the value to be a list:\n",
      "Map<String,List<String>>\n",
      "\n",
      "Example :\n",
      "class MapUtils\n",
      "{\n",
      "// Function to get Stream of String[]\n",
      "private static Stream<String[]> getMapStream()\n",
      "{\n",
      "    return Stream.of(new String[][] {\n",
      "            {\"CAR\", \"Audi\"},\n",
      "            {\"BIKE\", \"Harley Davidson\"},\n",
      "            {\"BIKE\", \"Pulsar\"}\n",
      "    });\n",
      "}\n",
      "\n",
      "// Program to convert Stream to Map in Java 8\n",
      "public static void main(String args[])\n",
      "{\n",
      "    // get stream of String[]\n",
      "    Stream<String[]> stream = getMapStream();\n",
      "\n",
      "    // construct a new map from the stream\n",
      "    Map<String, String> vehicle =\n",
      "            stream.collect(Collectors.toMap(e -> e[0], e -> e[1]));\n",
      "\n",
      "    System.out.println(vehicle);\n",
      "}\n",
      "}\n",
      "\n",
      "Output :\n",
      "java.lang.IllegalStateException: Duplicate key Harley Davidson\n",
      "\n",
      "I would like to have a way where \n",
      "\n",
      "I can operate on e->e[0] and e->e[1] to have the problem solved. Is that possible? For this I need an access of the current map object that's getting collected. I am not sure if that makes sense.\n",
      "A way where this can be achieved with Java 8 streams.\n",
      "\n",
      "Expected Output :\n",
      "{CAR=[Audi], BIKE=[Harley Davidson, Pulsar]}\n",
      "\n",
      "A:\n",
      "\n",
      "That's what groupingBy is for:\n",
      "Map<String,List<String>> vehicle = \n",
      "    stream.collect(Collectors.groupingBy(e -> e[0], \n",
      "                   Collectors.mapping(e -> e[1],\n",
      "                                      Collectors.toList())));\n",
      "\n",
      "Output map:\n",
      "{CAR=[Audi], BIKE=[Harley Davidson, Pulsar]}\n"
     ]
    }
   ],
   "source": [
    "# show dataset info\n",
    "print('d[\"train\"]:', d[\"train\"])\n",
    "print('d[\"train\"][text][0]:', d[\"train\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8727b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "save train and test dataset\n",
    "'''\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)\n",
    "\n",
    "print('begin to save dataset')\n",
    "dataset_to_text(d[\"train\"], \"./datasets/for_train/ds-train.txt\")\n",
    "dataset_to_text(d[\"test\"], \"./datasets/for_test/ds-test.txt\")\n",
    "print('dataset saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b78e69eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set params for training tokenizer\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "training tokenizer: set params for training\n",
    "'''\n",
    "\n",
    "special_tokens = [\n",
    "    \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"./datasets/for_train/ds-train.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = False\n",
    "model_path = \"models/pretrained-bert\"\n",
    "\n",
    "print('set params for training tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37102c-ae3c-45b9-881d-f61be6ed5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train tokenizer: train and save the tokenizer\n",
    "'''\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# initialize the WordPiece tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "# train the tokenizer\n",
    "print('begin to train a tokenizer')\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "print('done of training a tokenizer')\n",
    "# enable truncation up to the maximum 512 tokens\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "# make the directory if not already there\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer.save_model(model_path)\n",
    "print(f'save model to {model_path}')\n",
    "\n",
    "# dumping some of the tokenizer config to config file,\n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "configPath = os.path.join(model_path, \"config.json\")\n",
    "with open(configPath, \"w\") as f:\n",
    "    tokenizer_cfg = {\n",
    "        \"do_lower_case\": True,\n",
    "        \"unk_token\": \"[UNK]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"cls_token\": \"[CLS]\",\n",
    "        \"mask_token\": \"[MASK]\",\n",
    "        \"model_max_length\": max_length,\n",
    "        \"max_len\": max_length,\n",
    "    }\n",
    "    json.dump(tokenizer_cfg, f, indent=2)\n",
    "    print(f'save config to {configPath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359627a0-114a-4bd2-9c70-b32b2714e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from models/pretrained-bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cc6ec7c64a645caa9975ffc922366f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1862001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "preprocessing datasets\n",
    "'''\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "def encode_with_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "            max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "# when the tokenizer is trained and configured, load it as BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "print('load model from', model_path)\n",
    "\n",
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "# tokenizing the train dataset\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_dataset = d[\"test\"].map(encode, batched=True)\n",
    "if truncate_longer_samples:\n",
    "    # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    # remove other columns, and remain them as Python lists\n",
    "    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "print('done of preprocessing datasets')\n",
    "d = None # free memory\n",
    "\n",
    "# group texts\n",
    "from itertools import chain\n",
    "\n",
    "def group_texts(examples):\n",
    "    print(\"keys:\", examples.keys())\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts.\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last samples to ensure they can be batched perfectly.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "if not truncate_longer_samples:\n",
    "    print('begin to grouping texts')\n",
    "    train_dataset = train_dataset.map(group_texts, batched=True, desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    test_dataset = test_dataset.map(group_texts, batched=True, desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "\n",
    "    # convert to torch tensors\n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "print('done of preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321f3261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForMaskedLM(config\u001b[38;5;241m=\u001b[39mmodel_config)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# init data collator: randomly masks 20% of the token\u001b[39;00m\n\u001b[1;32m     10\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[0;32m---> 11\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m,\n\u001b[1;32m     12\u001b[0m     mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     mlm_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     17\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/results\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# save_total_limit=3,\u001b[39;00m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     32\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[1;32m     36\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "training model\n",
    "'''\n",
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "\n",
    "# init data collator: randomly masks 20% of the token\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.2\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models/results',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=10,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=1000,\n",
    "    save_steps=1000,\n",
    "    # load_best_model_at_end=True,\n",
    "    # save_total_limit=3,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b05bb9d",
   "metadata": {},
   "source": [
    "## 大语言模型结构\n",
    "\n",
    "![alt text](./C2/assets/GPT-2_model.png)\n",
    "\n",
    "区别：\n",
    "\n",
    "- 使用 pre-normalization：\n",
    "    - 在 多头自注意力（Multi-head Self Attention）之前\n",
    "    - 在 FFN 之前\n",
    "- 残留连接在 MSA 和 FFN 之后\n",
    "- Normalization：使用 RMSNorm（Root Mean Square Normalizing Function）\n",
    "- 激活函数：SwiGLU\n",
    "- 位置嵌入：RoPE\n",
    "\n",
    "RMSNorm 计算公式：\n",
    "\n",
    "$$\n",
    "RMS(a) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} a_i^2} + \\epsilon \\\\\n",
    "\\overline{a}_i = \\frac{a_i}{RMS(a)}\n",
    "$$\n",
    "\n",
    "也可以引入缩放因子：\n",
    "\n",
    "$$\n",
    "\\overline{a}_i = \\frac{a_i}{RMS(a)}g_i + b_i\n",
    "$$\n",
    "\n",
    "\n",
    "RoPE 借助了复数的思想，出发点是通过绝对位置编码的方式实现相对位置编码。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
